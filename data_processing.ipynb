{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from scipy import interpolate\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import plot_roc_curve,roc_curve,auc,roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce memory\n",
    "def reduce_memory(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2 #Total data memory\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#Process the record timestamp of data, get the day, year, and month, and convert ts to time format\n",
    "def process_dt(data):\n",
    "    data = reduce_memory(data) #reduce memory\n",
    "    data.dt=data.dt.apply(lambda x :datetime(x//10000,(x//100)%100,x%100).strftime('%Y-%m-%d'))\n",
    "    data['dt'] = pd.to_datetime(data.dt)\n",
    "    data = data.sort_values('dt').reset_index(drop = True) #Sort by sampling time\n",
    "    return data\n",
    "\n",
    "\n",
    "#calculate the init state change features\n",
    "def init_change(df):\n",
    "    # calculate the disk usage length\n",
    "    df['server_time'] = (df['dt'] - df['init_dt']).dt.days\n",
    "    diff_init = ['smart_7_normalized','smart_187raw','smart_199raw','smart_188_normalized','smart_195_normalized'] #变化特征列\n",
    "    \n",
    "    #calculate the change value\n",
    "    for col in diff_init:\n",
    "        df['init_diff_' + col] = df[col] - df['init_' + col]\n",
    "        df.drop('init_' + col, axis = 1, inplace = True)\n",
    "        \n",
    "    gc.collect() #release memory\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "# labelling\n",
    "def get_label(df, fault):\n",
    "    key_col = [\"manufacturer\", \"model\", \"serial_number\", \"dt\"]\n",
    "    fault[\"tag\"] = 1\n",
    "    fault = fault.drop_duplicates()\n",
    "    all_fault = pd.DataFrame()\n",
    "    for i in range(-1, 8):\n",
    "        tmp_fault = fault.copy()\n",
    "        tmp_fault[\"fault_time\"] -= timedelta(days=i)\n",
    "        all_fault = all_fault.append(tmp_fault, ignore_index=True)\n",
    "    all_fault = all_fault.drop_duplicates()\n",
    "    \n",
    "    df = df.merge(all_fault, on=[\"model\", \"serial_number\"], how=\"left\")\n",
    "    df[\"tag\"] = df[\"tag\"].fillna(0).astype(int)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    del df['fault_time']\n",
    "    return df\n",
    "    \n",
    "    \n",
    "# Remove features with a null value greater than 80% and nunique = 1 and fill nan with mean\n",
    "def drop_null_nunique(df):\n",
    "    nullCols = []\n",
    "    nuni = []\n",
    "    for i in tqdm([col for col in df.columns if col not in ['Unnamed: 0','serial_number','manufacturer','model']]):\n",
    "        nan = np.isnan(df[i]).mean()\n",
    "        if nan*100 >= 80:\n",
    "            nullCols.append(i)\n",
    "    df.drop(df[nullCols], axis=1, inplace=True)\n",
    "    \n",
    "    for i in tqdm([col for col in df.columns if col not in ['Unnamed: 0','serial_number','manufacturer','model']]):\n",
    "        df[i].fillna(df[i].mean(), inplace=True)\n",
    "        \n",
    "    for i in tqdm([col for col in df.columns if col not in ['Unnamed: 0','serial_number','manufacturer','model']]):\n",
    "        same = df[i].nunique()\n",
    "        if same == 1:\n",
    "            nuni.append(i)\n",
    "    df.drop(df[nuni], axis=1, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "    \n",
    "    \n",
    "# Collect data for March, April, May, June, and July, process time and fill nan value\n",
    "def generate_data():\n",
    "    #Collect data for March, April, May, June, and July\n",
    "    names = ['disk_sample_smart_log_201803.csv','disk_sample_smart_log_201804.csv',\n",
    "             'disk_sample_smart_log_201805.csv', 'disk_sample_smart_log_201806.csv',\n",
    "             'disk_sample_smart_log_201807.csv']\n",
    "    \n",
    "    names = ['/Users/yangxianjie/PycharmProjects/pythonProject/disk/raw_data/' + filename for filename in names]\n",
    "    paths = names\n",
    "    \n",
    "    chosen_col = ['serial_number', 'manufacturer', 'model', 'smart_5raw', 'smart_5_normalized', 'smart_197raw', 'smart_197_normalized', 'smart_198raw', 'smart_187_normalized', 'smart_198_normalized', 'smart_7_normalized', 'smart_187raw', 'smart_199raw', 'smart_188_normalized', 'smart_195_normalized', 'dt']\n",
    "    \n",
    "    data_34567 = pd.DataFrame()\n",
    "    for path in paths:\n",
    "        data = pd.read_csv(path) #read data\n",
    "        data = data[chosen_col] #Read specified characteristics\n",
    "        data = drop_null_nunique(data) #Remove the features whose null value is greater than 80% and nunique is 1, and read the specified feature\n",
    "        data = process_dt(data) #process time\n",
    "        data_34567 = data_34567.append(data).reset_index(drop = True) #Add the processed data to the data table\n",
    "        \n",
    "    data_34567 = data_34567.sort_values('dt').reset_index(drop = True) #Sort the data for the month of 34567 according to the sampling time\n",
    "    \n",
    "    print('save data to  /user_data/cube_data')\n",
    "    data_34567.to_pickle('/Users/yangxianjie/测试项目/AI/cube_data/data_34567_12.pkl')\n",
    "    del data_34567\n",
    "    gc.collect()\n",
    "\n",
    "   \n",
    "   \n",
    "# Generate serial file, record the earliest log of all disks\n",
    "def generate_serial():\n",
    "    # Collect data for all months\n",
    "    names = ['disk_sample_smart_log_201707.csv', 'disk_sample_smart_log_201708.csv', 'disk_sample_smart_log_201709.csv'\n",
    "        , 'disk_sample_smart_log_201710.csv', 'disk_sample_smart_log_201711.csv', 'disk_sample_smart_log_201712.csv'\n",
    "        , 'disk_sample_smart_log_201801.csv', 'disk_sample_smart_log_201802.csv', 'disk_sample_smart_log_201803.csv'\n",
    "        , 'disk_sample_smart_log_201804.csv', 'disk_sample_smart_log_201805.csv', 'disk_sample_smart_log_201806.csv'\n",
    "        , 'disk_sample_smart_log_201807.csv']\n",
    "    \n",
    "    names = ['/Users/yangxianjie/PycharmProjects/pythonProject/disk/raw_data/' + filename for filename in names]\n",
    "    paths = names\n",
    "    \n",
    "    serial = pd.DataFrame()\n",
    "    init_cols = ['smart_7_normalized','smart_187raw','smart_199raw','smart_188_normalized','smart_195_normalized','dt'] #Select some features\n",
    "    \n",
    "    #Splice features according to specified features and record the earliest log\n",
    "    for path in paths:\n",
    "        #print(path)\n",
    "        datas = pd.read_csv(path, chunksize = 500000)\n",
    "        for data in datas:\n",
    "            data = data[['serial_number', 'model'] + init_cols].sort_values('dt').drop_duplicates(['serial_number', 'model'])\n",
    "            serial = pd.concat((serial, data), axis = 0)\n",
    "            serial = serial.sort_values('dt').drop_duplicates(['serial_number', 'model']).reset_index(drop = True)\n",
    "            \n",
    "    serial = process_dt(serial)\n",
    "    \n",
    "    serial.rename(columns = dict(zip(init_cols, ['init_' + col for col in init_cols])), inplace = True) #Rename feature column\n",
    "    \n",
    "    serial.to_csv('/Users/yangxianjie/测试项目/AI/serial_col_12.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Generate the interpolated data set for the month of 34567\n",
    "    pd.DataFrame().dropna()\n",
    "    print(\"###### generate data ######\")\n",
    "    generate_data()\n",
    "    print('###### generate serial ######')\n",
    "    generate_serial()\n",
    "    \n",
    "    # read data\n",
    "    print('###### read data ######')\n",
    "    data_34567 = pd.read_pickle('/Users/yangxianjie/测试项目/AI/cube_data/data_34567_12.pkl')\n",
    "    \n",
    "    #read tag\n",
    "    print('###### read tag ######')\n",
    "    tag = pd.read_csv('/Users/yangxianjie/MSc Project/Dataset/disk_sample_fault_tag.csv')\n",
    "    tag['fault_time'] = pd.to_datetime(tag['fault_time']) #change time format\n",
    "    \n",
    "    #Some disks in the tag table have several failures on the same day\n",
    "    tag['tag'] = tag['tag'].astype(str)\n",
    "    tag = tag.groupby(['serial_number', 'fault_time', 'model'])['tag'].apply(lambda x: '|'.join(x)).reset_index() #Failure of spliced disks on the same day\n",
    "    \n",
    "    # read serial\n",
    "    print('###### read serial ######')\n",
    "    serial = pd.read_csv('/Users/yangxianjie/测试项目/AI/serial_col_12.csv')\n",
    "    serial.init_dt = pd.to_datetime(serial.init_dt)\n",
    "    \n",
    "    \n",
    "    #calculate the init state change features\n",
    "    print('###### calculate Initial change feature ######')\n",
    "    data_34567 = data_34567.merge(serial, how = 'left', on = ['serial_number', 'model']) #Merged with serial table\n",
    "    data_34567 = init_change(data_34567)\n",
    "    \n",
    "    # Label and screen samples at the same time\n",
    "    print('###### tag label ######')\n",
    "    data_34567 = get_label(data_34567, tag)\n",
    "    \n",
    "    # process serial_number\n",
    "    data_34567['serial_number'] = data_34567['serial_number'].apply(lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    print('###### save data for train ######')\n",
    "    data_34567.to_pickle('/Users/yangxianjie/测试项目/AI/cube_data/process_data_34567_12_feature_engineering.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "data_34567 = pd.read_pickle('/Users/yangxianjie/测试项目/AI/cube_data/process_data_34567_12_feature_engineering.pkl')\n",
    "\n",
    "# training features list\n",
    "smartcol = [col for col in data_34567.columns if col not in ['log_cumsum', 'Unnamed: 0', 'serial_number', 'model', 'dt', 'fault_time', 'tag', 'label', 'gap_bad_day', 'manufacturer', 'init_dt']]\n",
    "print('training features : ', smartcol)\n",
    "print(len(smartcol))\n",
    "\n",
    "data_34567 = data_34567.loc[data_34567['model'] == 1]\n",
    "\n",
    "# split the training dataset and test dataset\n",
    "train_x = data_34567.loc[data_34567['dt'].dt.month < 5]\n",
    "\n",
    "for i in smartcol:\n",
    "    train_x[i].fillna(0, inplace=True)\n",
    "print(train_x.isnull().sum())\n",
    "train_y = data_34567.loc[data_34567['dt'].dt.month < 5, 'tag']\n",
    "\n",
    "health = len(train_x[train_x['tag'] == 0])\n",
    "print('health disks = ', health)\n",
    "faild = len(train_x[train_x['tag'] == 1])\n",
    "print('faild disks = ', faild)\n",
    "\n",
    "val_x = data_34567.loc[data_34567['dt'].dt.month == 6]\n",
    "for i in smartcol:\n",
    "    val_x[i].fillna(0, inplace=True)\n",
    "print(val_x.isnull().sum())\n",
    "val_y = data_34567.loc[data_34567['dt'].dt.month == 6, 'tag']\n",
    "\n",
    "del data_34567\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def up_sampling(train_x, train_y, faild, ratio=3):\n",
    "    pos_sap_num = int(faild * ratio) \n",
    "    smo = BorderlineSMOTE(sampling_strategy={1: pos_sap_num}, random_state=1)\n",
    "    train_x, train_y = smo.fit_resample(train_x, train_y)\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def down_sampling(train_x, train_y, health, ratio=10):\n",
    "    neg_sap_num = int(health / ratio)\n",
    "    rus = RandomUnderSampler(sampling_strategy={0: neg_sap_num}, random_state=1)\n",
    "    train_x, train_y = rus.fit_resample(train_x, train_y)\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = up_sampling(train_x[smartcol], train_y, faild, 3)\n",
    "train_x, train_y = down_sampling(train_x, train_y, health, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.to_csv('/Users/yangxianjie/测试项目/AI/cube_data/train_x_12_feature_engineering.csv')\n",
    "train_y.to_csv('/Users/yangxianjie/测试项目/AI/cube_data/train_y_12_feature_engineering.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x[smartcol].to_csv('/Users/yangxianjie/测试项目/AI/cube_data/valid_x_12_feature_engineering.csv')\n",
    "val_y.to_csv('/Users/yangxianjie/测试项目/AI/cube_data/valid_y_12_feature_engineering.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
